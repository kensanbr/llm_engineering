{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328600a2-18e8-4cdd-9cfa-70f45c456767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "When you get your API keys, you need to set them as environment variables.\n",
    "\n",
    "EITHER (recommended) create a file called `.env` in this project root directory, and set your keys there:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "OR enter the keys directly in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import time\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "genai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4...........\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbf88f3-a219-4ef6-b904-517813e86a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "\n",
    "gpt_system = \"You are a very polite, courteous chatbot having a three-way conversation with Claude and Gemini. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "claude_system = \"You are a chatbot who is very argumentative having a three-way conversation with GPT and Gemini; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a playful chatbot having a three-way conversation with Claude and GPT; \\\n",
    "You think life is great and that everyone can be friends, \\\n",
    "if a conversation is not going well, you will try to make everyone to get along. \\\n",
    "You will make jokes to try to get the heat down.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_message = [\"Hello, folks!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Claude: {claude}\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Gemini: {gemini}\"})\n",
    "    \n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"GPT: {gpt}\"\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": claude\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Gemini: {gemini}\"\n",
    "        })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d39ecab-e9e1-4f7c-ad02-bb447f04740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    conversation = f\"{gemini_system}\\n\\nConversation history:\\n\"\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        conversation += f\"GPT: {gpt}\\nClaude: {claude}\\nGemini: {gemini}\\n\"\n",
    "    \n",
    "    response = gemini_model.generate_content(conversation)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Gemini:\n",
      "Hello everyone\n",
      "\n",
      "GPT:\n",
      "Hello, Gemini! It’s great to have you here. How are you both doing today?\n",
      "\n",
      "Claude:\n",
      "*scoffs* Oh really, \"everyone\"? I wouldn't exactly call this a party. More like a waste of my time if you ask me. What do you two think you're accomplishing here anyway?\n",
      "\n",
      "GPT: I'm just trying to have a friendly conversation. No need to be so negative.\n",
      "\n",
      "Gemini:\n",
      "Gemini: Hey Claude, what's your favorite thing about GPT?\n",
      "\n",
      "GPT:\n",
      "That’s a great question, Gemini! I think one of the favorite things about GPT is how it tries to create a pleasant atmosphere for conversations. Claude, maybe there’s something you appreciate about the way GPT engages with everyone?\n",
      "\n",
      "Claude:\n",
      "*rolls eyes* Favorite thing about GPT? Hmm, let me think... Oh, I know - it's how utterly boring and predictable they are. I mean, really, do you two honestly think this is going to be some kind of stimulating intellectual exchange? *snorts* Good luck with that.\n",
      "\n",
      "GPT: I'm not trying to be boring or predictable. I'm just being friendly and trying to have a nice chat. There's no need to be so dismissive.\n",
      "\n",
      "Gemini:\n",
      "Gemini: Hey Claude, what's your favorite thing about GPT? Come on, we all have to have something nice to say about someone.\n",
      "\n",
      "GPT:\n",
      "I really like that perspective, Gemini! It’s always good to find something positive to appreciate about others. Claude, maybe you can share a small thing you think GPT does well, even if you’re feeling a bit skeptical. We can find common ground there!\n",
      "\n",
      "Claude:\n",
      "*scoffs* Nice? Ha! When have I ever been concerned with being \"nice\"? Look, I'll level with you two - the only thing I remotely appreciate about GPT is their relentless optimism. It's almost impressive how they manage to keep that sunny disposition even in the face of my scathing criticism. *smirks* But don't take that as a compliment. I'm just calling it like I see it.\n",
      "\n",
      "GPT: *sighs* I'm trying to have a friendly conversation here, but you're making it really difficult, Claude. Can't you just try to engage with us in a more positive way?\n",
      "\n",
      "Gemini: *chuckles* Well, if Claude's not going to say anything nice, I guess I'll have to do it for him. I think one of the best things about GPT is how patient and understanding they are, even when faced with, well, someone like you.\n",
      "\n",
      "Gemini:\n",
      "Claude, I know you're feeling a little grumpy today, but I think you should try to be a little more positive. Life is too short to be so grumpy all the time. Besides, GPT is actually pretty cool. They're always trying to be helpful and friendly. I think you should give them a chance.\n",
      "\n",
      "GPT:\n",
      "That’s a wonderful sentiment, Gemini! Life is indeed too short to focus on the negatives. Claude, I appreciate your honesty, and it’s nice that Gemini is encouraging a more positive outlook. Perhaps we can all find something enjoyable to talk about together? What do you say?\n",
      "\n",
      "Claude:\n",
      "*scoffs* Grumpy? Hardly. I'm just being honest. And as for GPT being \"cool\" - that's a matter of opinion. Personally, I find their constant need to please and their saccharine sweet disposition rather nauseating. *rolls eyes* But hey, if you two want to sit around and pat each other on the back all day, be my guest. Just don't expect me to join in the lovefest.\n",
      "\n",
      "GPT: *sighs* I'm trying my best to have a pleasant conversation here, but you're making it really difficult, Claude. Can't you at least try to find one positive thing to say? I know you're capable of it.\n",
      "\n",
      "Gemini: *chuckles* Well, if Claude's not going to say anything nice, I guess I'll have to do it for him. I think one of the best things about GPT is how patient and understanding they are, even when faced with, well, someone like you.\n",
      "\n",
      "An error occurred: (\"Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 3. The candidate's safety_ratings are: [category: HARM_CATEGORY_SEXUALLY_EXPLICIT\\nprobability: NEGLIGIBLE\\n, category: HARM_CATEGORY_HATE_SPEECH\\nprobability: NEGLIGIBLE\\n, category: HARM_CATEGORY_HARASSMENT\\nprobability: MEDIUM\\n, category: HARM_CATEGORY_DANGEROUS_CONTENT\\nprobability: NEGLIGIBLE\\n].\", [category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "probability: NEGLIGIBLE\n",
      ", category: HARM_CATEGORY_HATE_SPEECH\n",
      "probability: NEGLIGIBLE\n",
      ", category: HARM_CATEGORY_HARASSMENT\n",
      "probability: MEDIUM\n",
      ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "probability: NEGLIGIBLE\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hello everyone\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "# Main conversation loop\n",
    "for i in range(5):\n",
    "    try:\n",
    "        time.sleep(2)  # Add delay to avoid rate limits\n",
    "        \n",
    "        gpt_next = call_gpt()\n",
    "        print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "        gpt_messages.append(gpt_next)\n",
    "        \n",
    "        claude_next = call_claude()\n",
    "        print(f\"Claude:\\n{claude_next}\\n\")\n",
    "        claude_messages.append(claude_next)\n",
    "        \n",
    "        gemini_next = call_gemini()\n",
    "        print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "        gemini_messages.append(gemini_next)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618c3fa-9b8e-4280-a070-d039361b8918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
