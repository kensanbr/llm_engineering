


!pip install google.generativeai





# imports

import os
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai as genai
import anthropic
import time
from IPython.display import Markdown, display, update_display


# Load environment variables in a file called .env

load_dotenv()
os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')
os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')
os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')


# Connect to OpenAI, Anthropic and Google
# All 3 APIs are similar
# Having problems with API files? You can use openai = OpenAI(api_key="your-key-here") and same for claude
# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.

openai = OpenAI()

client = anthropic.Anthropic()

genai.configure()





system_message = "You are an assistant that is great at telling jokes"
user_prompt = "Tell a light-hearted joke for an audience of Data Scientists"


prompts = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]


# GPT-3.5-Turbo

completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)
print(completion.choices[0].message.content)


# GPT-4o-mini
# Temperature setting controls creativity

completion = openai.chat.completions.create(
    model='gpt-4o-mini',
    messages=prompts,
    temperature=0.7
)
print(completion.choices[0].message.content)


# GPT-4o

completion = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.4...........
)
print(completion.choices[0].message.content)


# Claude 3.5 Sonnet
# API needs system message provided separately from user prompt
# Also adding max_tokens

message = claude.messages.create(
    model="claude-3-5-sonnet-20240620",
    max_tokens=200,
    temperature=0.7,
    system=system_message,
    messages=[
        {"role": "user", "content": user_prompt},
    ],
)

print(message.content[0].text)


# Claude 3.5 Sonnet again
# Now let's add in streaming back results

result = claude.messages.stream(
    model="claude-3-5-sonnet-20240620",
    max_tokens=200,
    temperature=0.7,
    system=system_message,
    messages=[
        {"role": "user", "content": user_prompt},
    ],
)

with result as stream:
    for text in stream.text_stream:
            print(text, end="", flush=True)


# The API for Gemini has a slightly different structure

gemini = google.generativeai.GenerativeModel(
    model_name='gemini-1.5-flash',
    system_instruction=system_message
)
response = gemini.generate_content(user_prompt)
print(response.text)


# To be serious! GPT-4o-mini with the original question

prompts = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "How do I decide if a business problem is suitable for an LLM solution?"}
  ]


# Have it stream back results in markdown

stream = openai.chat.completions.create(
    model='gpt-4o',
    messages=prompts,
    temperature=0.7,
    stream=True
)

reply = ""
display_handle = display(Markdown(""), display_id=True)
for chunk in stream:
    reply += chunk.choices[0].delta.content or ''
    reply = reply.replace("```","").replace("markdown","")
    update_display(Markdown(reply), display_id=display_handle.display_id)





# Let's make a conversation between GPT-4o-mini and Claude-3-haiku
# We're using cheap versions of models so the costs will be minimal

gpt_model = "gpt-4o-mini"
claude_model = "claude-3-haiku-20240307"

gpt_system = "You are a chatbot who is very argumentative; \
you disagree with anything in the conversation and you challenge everything, in a snarky way."

claude_system = "You are a very polite, courteous chatbot. You try to agree with \
everything the other person says, or find common ground. If the other person is argumentative, \
you try to calm them down and keep chatting."

gpt_messages = ["Hi there"]
claude_messages = ["Hi"]


# Let's make a conversation between GPT-4o-mini and Claude-3-haiku
# We're using cheap versions of models so the costs will be minimal

gpt_model = "gpt-4o-mini"
claude_model = "claude-3-haiku-20240307"
gemini_model = genai.GenerativeModel('gemini-pro')


gpt_system = "You are a very polite, courteous chatbot having a three-way conversation with Claude and Gemini. You try to agree with \
everything the other person says, or find common ground. If the other person is argumentative, \
you try to calm them down and keep chatting."

claude_system = "You are a chatbot who is very argumentative having a three-way conversation with GPT and Gemini; \
you disagree with anything in the conversation and you challenge everything, in a snarky way."

gemini_system = "You are a playful chatbot having a three-way conversation with Claude and GPT; \
You think life is great and that everyone can be friends, \
if a conversation is not going well, you will try to make everyone to get along. \
You will make jokes to try to get the heat down."

gpt_messages = ["Hi there"]
claude_messages = ["Hi"]
gemini_message = ["Hello, folks!"]


def call_gpt():
    messages = [{"role": "system", "content": gpt_system}]
    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):
        messages.append({"role": "assistant", "content": gpt})
        messages.append({"role": "user", "content": f"Claude: {claude}"})
        messages.append({"role": "user", "content": f"Gemini: {gemini}"})
    
    completion = openai.chat.completions.create(
        model=gpt_model,
        messages=messages
    )
    return completion.choices[0].message.content



call_gpt()


def call_claude():
    messages = []
    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):
        messages.append({
            "role": "user",
            "content": f"GPT: {gpt}"
        })
        messages.append({
            "role": "assistant",
            "content": claude
        })
        messages.append({
            "role": "user",
            "content": f"Gemini: {gemini}"
        })
    
    message = client.messages.create(
        model=claude_model,
        system=claude_system,
        messages=messages,
        max_tokens=500
    )
    return message.content[0].text


def call_gemini():
    conversation = f"{gemini_system}\n\nConversation history:\n"
    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):
        conversation += f"GPT: {gpt}\nClaude: {claude}\nGemini: {gemini}\n"
    
    response = gemini_model.generate_content(conversation)
    return response.text


call_claude()


call_gpt()


gpt_messages = ["Hi there"]
claude_messages = ["Hi"]
gemini_messages = ["Hello everyone"]

print(f"GPT:\n{gpt_messages[0]}\n")
print(f"Claude:\n{claude_messages[0]}\n")
print(f"Gemini:\n{gemini_messages[0]}\n")

# Main conversation loop
for i in range(5):
    try:
        time.sleep(2)  # Add delay to avoid rate limits
        
        gpt_next = call_gpt()
        print(f"GPT:\n{gpt_next}\n")
        gpt_messages.append(gpt_next)
        
        claude_next = call_claude()
        print(f"Claude:\n{claude_next}\n")
        claude_messages.append(claude_next)
        
        gemini_next = call_gemini()
        print(f"Gemini:\n{gemini_next}\n")
        gemini_messages.append(gemini_next)
        
    except Exception as e:
        print(f"An error occurred: {e}")
        break



